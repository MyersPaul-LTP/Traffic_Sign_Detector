{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "#from tensorflow.contrib.layers import flatten\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from bayes_opt.logger import JSONLogger\n",
    "from bayes_opt.event import Events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering German Traffic Sign Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning file download...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./traffic-signs-data.zip', <http.client.HTTPMessage at 0x7efd725c5650>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Downloading Dataset\n",
    "import urllib.request\n",
    "\n",
    "print('Beginning file download...')\n",
    "\n",
    "url = 'https://s3-us-west-1.amazonaws.com/udacity-selfdrivingcar/traffic-signs-data.zip'\n",
    "\n",
    "urllib.request.urlretrieve(url, './traffic-signs-data.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning file unzip\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['train.p',\n",
       " 'traffic-signs-data.zip',\n",
       " '.ipynb_checkpoints',\n",
       " 'logs.json',\n",
       " '.git',\n",
       " '.gitignore',\n",
       " 'Traffic_Sign_Detection.ipynb',\n",
       " 'test.p',\n",
       " 'valid.p']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Unizping Dataset\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "\n",
    "print('Beginning file unzip')\n",
    "\n",
    "zip_ref = zipfile.ZipFile('./traffic-signs-data.zip', 'r')\n",
    "zip_ref.extractall('./')\n",
    "zip_ref.close()\n",
    "\n",
    "print('Done')\n",
    "os.listdir(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "# Visualizations will be shown in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = './train.p'\n",
    "validation_file= './valid.p'\n",
    "testing_file = './test.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=to_categorical(y_train)\n",
    "y_valid=to_categorical(y_valid)\n",
    "y_test=to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 34799\n",
      "Number of testing examples = 12630\n",
      "Number of validation examples = 4410\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 2\n"
     ]
    }
   ],
   "source": [
    "### Replace each question mark with the appropriate value. \n",
    "### Use python, pandas or numpy methods rather than hard coding the results\n",
    "\n",
    "# TODO: Number of training examples\n",
    "n_train = X_train.shape[0]\n",
    "# TODO: Number of validation examples\n",
    "n_validation = X_valid.shape[0]\n",
    "# TODO: Number of testing examples.\n",
    "n_test = X_test.shape[0]\n",
    "# TODO: What's the shape of an traffic sign image?\n",
    "image_shape = X_train.shape[1:]\n",
    "\n",
    "# TODO: How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(np.unique(y_train))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Number of validation examples =\", n_validation)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualizations will be shown in the notebook.\n",
    "# get_ipython().magic('matplotlib inline')\n",
    "\n",
    "# def draw_images_examples(image_array, grid_x, grid_y, title):\n",
    "#     fig = plt.figure(figsize=(grid_x,grid_y))\n",
    "#     fig.suptitle(title, fontsize=20)\n",
    " \n",
    "#     for i in range(1,grid_y*grid_x+1):\n",
    "#         index = random.randint(0, len(image_array))\n",
    "#         image = image_array[index].squeeze()\n",
    "       \n",
    "#         plt.subplot(grid_y,grid_x,i)\n",
    "#         plt.imshow(image)\n",
    "        \n",
    "# draw_images_examples(X_train, 16, 4, 'Examples of images from training set')\n",
    "    \n",
    "\n",
    "# fig = plt.figure(figsize=(12,4))\n",
    "# n, bins, patches = plt.hist(y_train, n_classes)\n",
    "# plt.xlabel('Labels')\n",
    "# plt.ylabel('No. of samples')\n",
    "# plt.title('Histogram of training samples')\n",
    "\n",
    "# X_train_one_label = X_train[np.where(y_train==0)]\n",
    "# draw_images_examples(X_train_one_label, 16, 4, 'Examples of images of the same type - Speed limit (20km/h)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "# from keras.layers import Dense, Dropout, Flatten, Input\n",
    "# from keras.layers import Conv2D, MaxPooling2D\n",
    "# from keras.layers import BatchNormalization\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_num_units = 2048\n",
    "hidden_num_units1 = 1024\n",
    "hidden_num_units2 = 128\n",
    "output_num_units = 43\n",
    "pool_size = (2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Black_Box(node0,node1,node2,node3,node4,node5,epochs,Drop0,Drop1,Drop2,Drop3,Drop4,Drop5,batch_size,LR):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(keras.layers.Conv2D(int(node0), (3, 3), activation='relu', input_shape=(32,32,3), padding='same'))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(int(node1), (3, 3), activation='relu', padding='same'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "    model.add(keras.layers.Dropout(int(Drop0)))\n",
    "\n",
    "\n",
    "\n",
    "    model.add(keras.layers.Conv2D(int(node2), (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(int(node3), (3, 3), activation='relu', padding='same'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "    model.add(keras.layers.Dropout(int(Drop1)))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(int(node4), (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "    model.add(keras.layers.Conv2D(int(node5), (3, 3), activation='relu', padding='same'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "    model.add(keras.layers.Dropout(int(Drop2)))\n",
    "    model.add(keras.layers.Flatten())\n",
    "\n",
    "    model.add(keras.layers.Dense(units=hidden_num_units, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(int(Drop3)))\n",
    "\n",
    "    model.add(keras.layers.Dense(units=hidden_num_units1, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(int(Drop4)))\n",
    "\n",
    "    model.add(keras.layers.Dense(units=hidden_num_units2, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(int(Drop5)))\n",
    "\n",
    "    model.add(keras.layers.Dense(units=output_num_units, input_dim=hidden_num_units, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(lr=LR), metrics=['accuracy'])\n",
    "    trained_model_conv = model.fit(X_train, y_train, epochs=int(epochs), batch_size=int(batch_size), validation_data=(X_valid, y_valid),verbose=1)\n",
    "    percentage=model.evaluate (X_train,y_train)[1]*100\n",
    "    \n",
    "#     number_of_equal_elements = np.sum(y_train==model.predict(X_train).round())\n",
    "#     total_elements = np.multiply(y_train.shape)\n",
    "#     percentage = number_of_equal_elements/total_elements\n",
    "    return percentage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds={'node0':(64,128),\n",
    "        'node1':(64,128),\n",
    "        'node2':(32,128),\n",
    "        'node3':(16,64),\n",
    "        'node4':(16,64),\n",
    "        'node5':(16,32),\n",
    "        'epochs':(10,20),\n",
    "         'Drop0':(0.1,0.5),\n",
    "         'Drop1':(0.1,0.5),\n",
    "         'Drop2':(0.1,0.5),\n",
    "         'Drop3':(0.1,0.5),\n",
    "         'Drop4':(0.1,0.5),\n",
    "         'Drop5':(0.1,0.5),\n",
    "         'batch_size':(100,400),\n",
    "         'LR':(0.0005,0.0002)\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "    f=Black_Box,\n",
    "    pbounds=bounds,\n",
    "    random_state=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = JSONLogger(path=\"./logs.json\")\n",
    "optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Train on 34799 samples, validate on 4410 samples\n",
      "Epoch 1/13\n",
      "34799/34799 [==============================] - 239s 7ms/sample - loss: 1.7249 - acc: 0.5475 - val_loss: 1.1175 - val_acc: 0.7274\n",
      "Epoch 2/13\n",
      "34799/34799 [==============================] - 240s 7ms/sample - loss: 0.3080 - acc: 0.9145 - val_loss: 0.8614 - val_acc: 0.8463\n",
      "Epoch 3/13\n",
      "34799/34799 [==============================] - 229s 7ms/sample - loss: 0.1389 - acc: 0.9629 - val_loss: 0.7989 - val_acc: 0.8714\n",
      "Epoch 4/13\n",
      "34799/34799 [==============================] - 240s 7ms/sample - loss: 0.0714 - acc: 0.9809 - val_loss: 0.6480 - val_acc: 0.8823\n",
      "Epoch 5/13\n",
      "34799/34799 [==============================] - 250s 7ms/sample - loss: 0.0642 - acc: 0.9829 - val_loss: 0.6683 - val_acc: 0.8927\n",
      "Epoch 6/13\n",
      "34799/34799 [==============================] - 245s 7ms/sample - loss: 0.0457 - acc: 0.9879 - val_loss: 0.8935 - val_acc: 0.8982\n",
      "Epoch 7/13\n",
      "34799/34799 [==============================] - 239s 7ms/sample - loss: 0.0270 - acc: 0.9937 - val_loss: 0.7758 - val_acc: 0.8971\n",
      "Epoch 8/13\n",
      "34799/34799 [==============================] - 239s 7ms/sample - loss: 0.0444 - acc: 0.9885 - val_loss: 0.5371 - val_acc: 0.9211\n",
      "Epoch 9/13\n",
      "34799/34799 [==============================] - 244s 7ms/sample - loss: 0.0263 - acc: 0.9932 - val_loss: 0.5798 - val_acc: 0.9091\n",
      "Epoch 10/13\n",
      "34799/34799 [==============================] - 243s 7ms/sample - loss: 0.0325 - acc: 0.9911 - val_loss: 0.7030 - val_acc: 0.9098\n",
      "Epoch 11/13\n",
      "34799/34799 [==============================] - 249s 7ms/sample - loss: 0.0481 - acc: 0.9880 - val_loss: 0.5293 - val_acc: 0.9358\n",
      "Epoch 12/13\n",
      "34799/34799 [==============================] - 242s 7ms/sample - loss: 0.0175 - acc: 0.9953 - val_loss: 0.6036 - val_acc: 0.9254\n",
      "Epoch 13/13\n",
      "34799/34799 [==============================] - 238s 7ms/sample - loss: 0.0281 - acc: 0.9933 - val_loss: 0.5890 - val_acc: 0.9238\n",
      "34799/34799 [==============================] - 108s 3ms/sample - loss: 0.0420 - acc: 0.9919\n",
      "Train on 34799 samples, validate on 4410 samples\n",
      "Epoch 1/16\n",
      "34799/34799 [==============================] - 263s 8ms/sample - loss: 2.0539 - acc: 0.4948 - val_loss: 1.6958 - val_acc: 0.6717\n",
      "Epoch 2/16\n",
      "34799/34799 [==============================] - 261s 7ms/sample - loss: 0.5146 - acc: 0.8671 - val_loss: 1.0577 - val_acc: 0.7998\n",
      "Epoch 3/16\n",
      "34799/34799 [==============================] - 263s 8ms/sample - loss: 0.2316 - acc: 0.9417 - val_loss: 1.0232 - val_acc: 0.8111\n",
      "Epoch 4/16\n",
      "34799/34799 [==============================] - 255s 7ms/sample - loss: 0.1257 - acc: 0.9670 - val_loss: 0.9819 - val_acc: 0.8492\n",
      "Epoch 5/16\n",
      "34799/34799 [==============================] - 253s 7ms/sample - loss: 0.1129 - acc: 0.9710 - val_loss: 0.9037 - val_acc: 0.8692\n",
      "Epoch 6/16\n",
      "34799/34799 [==============================] - 255s 7ms/sample - loss: 0.0737 - acc: 0.9817 - val_loss: 0.9972 - val_acc: 0.8556\n",
      "Epoch 7/16\n",
      "34799/34799 [==============================] - 263s 8ms/sample - loss: 0.0365 - acc: 0.9900 - val_loss: 0.8032 - val_acc: 0.8732\n",
      "Epoch 8/16\n",
      "34799/34799 [==============================] - 258s 7ms/sample - loss: 0.0379 - acc: 0.9892 - val_loss: 0.9798 - val_acc: 0.8633\n",
      "Epoch 9/16\n",
      "34799/34799 [==============================] - 256s 7ms/sample - loss: 0.0311 - acc: 0.9920 - val_loss: 1.0325 - val_acc: 0.8785\n",
      "Epoch 10/16\n",
      "34799/34799 [==============================] - 259s 7ms/sample - loss: 0.0321 - acc: 0.9924 - val_loss: 0.9326 - val_acc: 0.8948\n",
      "Epoch 11/16\n",
      "34799/34799 [==============================] - 256s 7ms/sample - loss: 0.0237 - acc: 0.9938 - val_loss: 0.8722 - val_acc: 0.8839\n",
      "Epoch 12/16\n",
      "34799/34799 [==============================] - 257s 7ms/sample - loss: 0.0254 - acc: 0.9935 - val_loss: 0.8617 - val_acc: 0.8882\n",
      "Epoch 13/16\n",
      "34799/34799 [==============================] - 254s 7ms/sample - loss: 0.0155 - acc: 0.9955 - val_loss: 0.7562 - val_acc: 0.9000\n",
      "Epoch 14/16\n",
      "34799/34799 [==============================] - 260s 7ms/sample - loss: 0.0123 - acc: 0.9971 - val_loss: 0.9709 - val_acc: 0.8807\n",
      "Epoch 15/16\n",
      "34799/34799 [==============================] - 260s 7ms/sample - loss: 0.0326 - acc: 0.9920 - val_loss: 0.9033 - val_acc: 0.8830\n",
      "Epoch 16/16\n",
      "34799/34799 [==============================] - 264s 8ms/sample - loss: 0.0243 - acc: 0.9939 - val_loss: 0.9305 - val_acc: 0.8787\n",
      "34799/34799 [==============================] - 129s 4ms/sample - loss: 0.0223 - acc: 0.9941\n",
      "Train on 34799 samples, validate on 4410 samples\n",
      "Epoch 1/10\n",
      "34799/34799 [==============================] - 277s 8ms/sample - loss: 2.1807 - acc: 0.4410 - val_loss: 1.4702 - val_acc: 0.6220\n",
      "Epoch 2/10\n",
      "34799/34799 [==============================] - 274s 8ms/sample - loss: 0.5969 - acc: 0.8333 - val_loss: 0.9628 - val_acc: 0.7551\n",
      "Epoch 3/10\n",
      "34799/34799 [==============================] - 275s 8ms/sample - loss: 0.2484 - acc: 0.9334 - val_loss: 0.9927 - val_acc: 0.7864\n",
      "Epoch 4/10\n",
      "34799/34799 [==============================] - 271s 8ms/sample - loss: 0.1535 - acc: 0.9580 - val_loss: 0.8540 - val_acc: 0.8406\n",
      "Epoch 5/10\n",
      "34799/34799 [==============================] - 272s 8ms/sample - loss: 0.0895 - acc: 0.9766 - val_loss: 0.8224 - val_acc: 0.8569\n",
      "Epoch 6/10\n",
      "34799/34799 [==============================] - 274s 8ms/sample - loss: 0.0496 - acc: 0.9874 - val_loss: 0.7127 - val_acc: 0.8669\n",
      "Epoch 7/10\n",
      "34799/34799 [==============================] - 272s 8ms/sample - loss: 0.0513 - acc: 0.9868 - val_loss: 0.9285 - val_acc: 0.8594\n",
      "Epoch 8/10\n",
      "34799/34799 [==============================] - 272s 8ms/sample - loss: 0.0413 - acc: 0.9891 - val_loss: 0.7900 - val_acc: 0.8803\n",
      "Epoch 9/10\n",
      "34799/34799 [==============================] - 270s 8ms/sample - loss: 0.0253 - acc: 0.9933 - val_loss: 0.6674 - val_acc: 0.8952\n",
      "Epoch 10/10\n",
      "34799/34799 [==============================] - 273s 8ms/sample - loss: 0.0224 - acc: 0.9941 - val_loss: 0.7543 - val_acc: 0.8896\n",
      "34799/34799 [==============================] - 148s 4ms/sample - loss: 0.0274 - acc: 0.9928\n",
      "Train on 34799 samples, validate on 4410 samples\n",
      "Epoch 1/12\n",
      "34799/34799 [==============================] - 202s 6ms/sample - loss: 1.6312 - acc: 0.5831 - val_loss: 1.2458 - val_acc: 0.7118\n",
      "Epoch 2/12\n",
      "34799/34799 [==============================] - 200s 6ms/sample - loss: 0.3246 - acc: 0.9108 - val_loss: 0.8154 - val_acc: 0.8338\n",
      "Epoch 3/12\n",
      "34799/34799 [==============================] - 204s 6ms/sample - loss: 0.1477 - acc: 0.9603 - val_loss: 0.7487 - val_acc: 0.8671\n",
      "Epoch 4/12\n",
      "34799/34799 [==============================] - 201s 6ms/sample - loss: 0.0994 - acc: 0.9747 - val_loss: 0.5898 - val_acc: 0.8846\n",
      "Epoch 5/12\n",
      "34799/34799 [==============================] - 203s 6ms/sample - loss: 0.0587 - acc: 0.9851 - val_loss: 0.5642 - val_acc: 0.9034\n",
      "Epoch 6/12\n",
      "34799/34799 [==============================] - 203s 6ms/sample - loss: 0.0471 - acc: 0.9880 - val_loss: 0.6747 - val_acc: 0.8893\n",
      "Epoch 7/12\n",
      "34799/34799 [==============================] - 204s 6ms/sample - loss: 0.0404 - acc: 0.9896 - val_loss: 0.7442 - val_acc: 0.8918\n",
      "Epoch 8/12\n",
      "34799/34799 [==============================] - 202s 6ms/sample - loss: 0.0337 - acc: 0.9913 - val_loss: 0.5349 - val_acc: 0.9014\n",
      "Epoch 9/12\n",
      "34799/34799 [==============================] - 202s 6ms/sample - loss: 0.0264 - acc: 0.9933 - val_loss: 0.7836 - val_acc: 0.8939\n",
      "Epoch 10/12\n",
      "34799/34799 [==============================] - 205s 6ms/sample - loss: 0.0260 - acc: 0.9930 - val_loss: 0.5619 - val_acc: 0.9050\n",
      "Epoch 11/12\n",
      "34799/34799 [==============================] - 205s 6ms/sample - loss: 0.0213 - acc: 0.9943 - val_loss: 0.8191 - val_acc: 0.9054\n",
      "Epoch 12/12\n",
      "34799/34799 [==============================] - 199s 6ms/sample - loss: 0.0204 - acc: 0.9951 - val_loss: 0.6251 - val_acc: 0.9057\n",
      "34799/34799 [==============================] - 89s 3ms/sample - loss: 0.0272 - acc: 0.9922\n",
      "Train on 34799 samples, validate on 4410 samples\n",
      "Epoch 1/19\n",
      "34799/34799 [==============================] - 267s 8ms/sample - loss: 2.0442 - acc: 0.4857 - val_loss: 1.4922 - val_acc: 0.6658\n",
      "Epoch 2/19\n",
      "34799/34799 [==============================] - 262s 8ms/sample - loss: 0.4616 - acc: 0.8754 - val_loss: 0.8624 - val_acc: 0.8088\n",
      "Epoch 3/19\n",
      "34799/34799 [==============================] - 259s 7ms/sample - loss: 0.1874 - acc: 0.9505 - val_loss: 1.1384 - val_acc: 0.7853\n",
      "Epoch 4/19\n",
      "34799/34799 [==============================] - 264s 8ms/sample - loss: 0.1253 - acc: 0.9672 - val_loss: 0.7894 - val_acc: 0.8612\n",
      "Epoch 5/19\n",
      "34799/34799 [==============================] - 265s 8ms/sample - loss: 0.0824 - acc: 0.9786 - val_loss: 0.7904 - val_acc: 0.8689\n",
      "Epoch 6/19\n",
      "34799/34799 [==============================] - 267s 8ms/sample - loss: 0.0483 - acc: 0.9873 - val_loss: 1.0669 - val_acc: 0.8612\n",
      "Epoch 7/19\n",
      "34799/34799 [==============================] - 262s 8ms/sample - loss: 0.0454 - acc: 0.9871 - val_loss: 0.6296 - val_acc: 0.8859\n",
      "Epoch 8/19\n",
      "34799/34799 [==============================] - 262s 8ms/sample - loss: 0.0348 - acc: 0.9903 - val_loss: 0.8444 - val_acc: 0.8791\n",
      "Epoch 9/19\n",
      "34799/34799 [==============================] - 259s 7ms/sample - loss: 0.0433 - acc: 0.9896 - val_loss: 0.7384 - val_acc: 0.8905\n",
      "Epoch 10/19\n",
      "34799/34799 [==============================] - 263s 8ms/sample - loss: 0.0192 - acc: 0.9953 - val_loss: 0.8281 - val_acc: 0.8939\n",
      "Epoch 11/19\n",
      "34799/34799 [==============================] - 260s 7ms/sample - loss: 0.0251 - acc: 0.9934 - val_loss: 0.5341 - val_acc: 0.9150\n",
      "Epoch 12/19\n",
      "34799/34799 [==============================] - 265s 8ms/sample - loss: 0.0293 - acc: 0.9919 - val_loss: 0.7617 - val_acc: 0.8973\n",
      "Epoch 13/19\n",
      "34799/34799 [==============================] - 262s 8ms/sample - loss: 0.0218 - acc: 0.9937 - val_loss: 0.5675 - val_acc: 0.9218\n",
      "Epoch 14/19\n",
      "34799/34799 [==============================] - 261s 7ms/sample - loss: 0.0181 - acc: 0.9954 - val_loss: 0.7375 - val_acc: 0.8971\n",
      "Epoch 15/19\n",
      "34799/34799 [==============================] - 265s 8ms/sample - loss: 0.0323 - acc: 0.9919 - val_loss: 0.5971 - val_acc: 0.9259\n",
      "Epoch 16/19\n",
      "34799/34799 [==============================] - 267s 8ms/sample - loss: 0.0146 - acc: 0.9959 - val_loss: 2.1148 - val_acc: 0.8361\n",
      "Epoch 17/19\n",
      "34799/34799 [==============================] - 268s 8ms/sample - loss: 0.0645 - acc: 0.9844 - val_loss: 0.6148 - val_acc: 0.9188\n",
      "Epoch 18/19\n",
      "34799/34799 [==============================] - 263s 8ms/sample - loss: 0.0073 - acc: 0.9985 - val_loss: 0.5373 - val_acc: 0.9304\n",
      "Epoch 19/19\n",
      "34799/34799 [==============================] - 264s 8ms/sample - loss: 0.0130 - acc: 0.9967 - val_loss: 0.8363 - val_acc: 0.9202\n",
      "34799/34799 [==============================] - 128s 4ms/sample - loss: 0.0100 - acc: 0.9970\n",
      "Train on 34799 samples, validate on 4410 samples\n",
      "Epoch 1/17\n",
      "34799/34799 [==============================] - 129s 4ms/sample - loss: 3.0559 - acc: 0.2925 - val_loss: 2.2925 - val_acc: 0.4605\n",
      "Epoch 2/17\n",
      "34799/34799 [==============================] - 129s 4ms/sample - loss: 1.1701 - acc: 0.6879 - val_loss: 1.6969 - val_acc: 0.6274\n",
      "Epoch 3/17\n",
      "34799/34799 [==============================] - 131s 4ms/sample - loss: 0.5464 - acc: 0.8530 - val_loss: 1.2533 - val_acc: 0.7224\n",
      "Epoch 4/17\n",
      "34799/34799 [==============================] - 127s 4ms/sample - loss: 0.3021 - acc: 0.9213 - val_loss: 1.2371 - val_acc: 0.7628\n",
      "Epoch 5/17\n",
      "34799/34799 [==============================] - 132s 4ms/sample - loss: 0.1904 - acc: 0.9490 - val_loss: 1.2540 - val_acc: 0.7916\n",
      "Epoch 6/17\n",
      "34799/34799 [==============================] - 128s 4ms/sample - loss: 0.1296 - acc: 0.9661 - val_loss: 1.1821 - val_acc: 0.7914\n",
      "Epoch 7/17\n",
      "34799/34799 [==============================] - 136s 4ms/sample - loss: 0.0941 - acc: 0.9762 - val_loss: 1.1930 - val_acc: 0.7980\n",
      "Epoch 8/17\n",
      "34799/34799 [==============================] - 127s 4ms/sample - loss: 0.0818 - acc: 0.9786 - val_loss: 1.0517 - val_acc: 0.8054\n",
      "Epoch 9/17\n",
      "34799/34799 [==============================] - 134s 4ms/sample - loss: 0.0631 - acc: 0.9844 - val_loss: 1.1001 - val_acc: 0.8272\n",
      "Epoch 10/17\n",
      "34799/34799 [==============================] - 132s 4ms/sample - loss: 0.0437 - acc: 0.9892 - val_loss: 1.4794 - val_acc: 0.7993\n",
      "Epoch 11/17\n",
      "34799/34799 [==============================] - 127s 4ms/sample - loss: 0.0371 - acc: 0.9913 - val_loss: 1.4021 - val_acc: 0.8265\n",
      "Epoch 12/17\n",
      "34799/34799 [==============================] - 128s 4ms/sample - loss: 0.0305 - acc: 0.9923 - val_loss: 1.1800 - val_acc: 0.8388\n",
      "Epoch 13/17\n",
      "34799/34799 [==============================] - 127s 4ms/sample - loss: 0.0422 - acc: 0.9899 - val_loss: 1.3299 - val_acc: 0.8234\n",
      "Epoch 14/17\n",
      "34799/34799 [==============================] - 132s 4ms/sample - loss: 0.0256 - acc: 0.9941 - val_loss: 1.3951 - val_acc: 0.8546\n",
      "Epoch 15/17\n",
      "34799/34799 [==============================] - 135s 4ms/sample - loss: 0.0226 - acc: 0.9943 - val_loss: 1.3373 - val_acc: 0.8340\n",
      "Epoch 16/17\n",
      "34799/34799 [==============================] - 133s 4ms/sample - loss: 0.0179 - acc: 0.9955 - val_loss: 1.2837 - val_acc: 0.8497\n",
      "Epoch 17/17\n",
      "34799/34799 [==============================] - 129s 4ms/sample - loss: 0.0149 - acc: 0.9964 - val_loss: 1.3672 - val_acc: 0.8440\n",
      "34799/34799 [==============================] - 76s 2ms/sample - loss: 0.0124 - acc: 0.9976\n"
     ]
    }
   ],
   "source": [
    "optimizer.maximize(n_iter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "batch_size = 200\n",
    "pool_size = (2,2)\n",
    "#list_images /= 255.0\n",
    "input_shape = Input(shape=(32, 32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.Conv2D(128, (3, 3), activation='relu', input_shape=(32,32,3), padding='same'))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "          \n",
    "\n",
    "          \n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "    \n",
    "model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "\n",
    "model.add(keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=pool_size))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Flatten())\n",
    "          \n",
    "model.add(keras.layers.Dense(units=hidden_num_units, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "          \n",
    "model.add(keras.layers.Dense(units=hidden_num_units1, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "          \n",
    "model.add(keras.layers.Dense(units=hidden_num_units2, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.3))\n",
    "          \n",
    "model.add(keras.layers.Dense(units=output_num_units, input_dim=hidden_num_units, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(lr=1e-4), metrics=['accuracy'])\n",
    "#trained_model_conv = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_valid, y_valid),verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of the learning history of the trained CNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "# summarize history for accuracy\n",
    "plt.subplot(211)\n",
    "plt.plot(trained_model_conv.history['acc'])\n",
    "#plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "# summarize history for loss\n",
    "plt.subplot(212)\n",
    "plt.plot(trained_model_conv.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=model.evaluate(X_test, y_test)\n",
    "print('loss:', score[0])\n",
    "print('accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #import tensorflow as tf\n",
    "# #from tensorflow.contrib.layers import flatten\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def grayscale(img):    \n",
    "#     return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)[:,:,None]\n",
    "\n",
    "# def normalize(value):\n",
    "#     return value / 255 * 2 - 1\n",
    "\n",
    "# def preprocess_image(image):\n",
    "#     img = grayscale(image)\n",
    "#     img = normalize(img)\n",
    "#     return img\n",
    "\n",
    "\n",
    "# def preprocess_batch(images):\n",
    "#     imgs = np.zeros(shape=images.shape)\n",
    "#     processed_image_depth = preprocess_image(images[0]).shape[2]\n",
    "#     imgs = imgs[:,:,:,0:processed_image_depth]\n",
    "#     for i in tqdm(range(images.shape[0])):\n",
    "#         imgs[i] = preprocess_image(images[i])        \n",
    "#     return imgs\n",
    "\n",
    "# X_train_processed = preprocess_batch(X_train)\n",
    "# X_valid_processed = preprocess_batch(X_valid)\n",
    "# X_test_processed = preprocess_batch(X_test)\n",
    "\n",
    "# no_test_image = 10000\n",
    "\n",
    "# sample_image = X_train[no_test_image]\n",
    "# sample_image_processed = grayscale(X_train[no_test_image])\n",
    "# fig=plt.figure(figsize=(16,3))\n",
    "# sub=plt.subplot(131)\n",
    "# sub.set_title(\"Original image\")\n",
    "# plt.imshow(sample_image)\n",
    "# sub=plt.subplot(132)\n",
    "# sub.set_title(\"Preprocessed image\")\n",
    "# plt.imshow(sample_image_processed.squeeze(), cmap='gray')\n",
    "\n",
    "# print(\"Sample image dimension BEFORE processing: {}\".format(sample_image.shape))\n",
    "# print(\"Sample image dimension AFTER processing: {}\".format(sample_image_processed.shape))\n",
    "\n",
    "# image_depth = X_train_processed.shape[3]\n",
    "\n",
    "# sample_image = X_train[no_test_image]\n",
    "# dim1 = sample_image.shape[0]\n",
    "# dim2 = sample_image.shape[1]\n",
    "# dim3 = sample_image.shape[2]\n",
    "# sample_image_reshaped = np.reshape(sample_image, dim1*dim2*dim3)\n",
    "# plt.figure(figsize=(16,3))\n",
    "# sub=plt.subplot(131)\n",
    "# sub.set_title(\"Original image histogram\")\n",
    "# n, bins, patches = plt.hist(sample_image_reshaped, 255)\n",
    "\n",
    "# sample_image_processed = X_train_processed[no_test_image]\n",
    "# dim1 = sample_image_processed.shape[0]\n",
    "# dim2 = sample_image_processed.shape[1]\n",
    "# dim3 = sample_image_processed.shape[2]\n",
    "# sample_image_processed_reshaped = np.reshape(sample_image_processed, dim1*dim2*dim3)\n",
    "# sub=plt.subplot(132)\n",
    "# sub.set_title(\"Preprocessed image histogram\")\n",
    "# n, bins, patches = plt.hist(sample_image_processed_reshaped,255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Articles\n",
    "- https://github.com/tomaszkacmajor/CarND-Traffic-Sign-Classifier-P2/blob/master/Traffic_Sign_Classifier.ipynb\n",
    "- https://medium.com/typeiqs/traffic-sign-recognition-aa38d699ac9 \n",
    "- https://medium.com/@sdoshi579/convolutional-neural-network-learn-and-apply-3dac9acfe2b6\n",
    "- https://towardsdatascience.com/traffic-sign-detection-using-convolutional-neural-network-660fb32fe90e    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
